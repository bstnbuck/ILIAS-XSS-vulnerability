# !/usr/bin/python3 

# ILIAS-XSS-vulnerability
# Copyright 2020/2021, Marius Binal and Bastian Buck, All rights reserved. 
# Contacts: Github: https://github.com/marius56; https://github.com/bstnbuck

import datetime
import os
from os.path import exists
import re
import csv
import argparse
from getpass import getpass
import pymysql as db
from colorama import Style, Fore, init
from bs4 import BeautifulSoup
from prettytable import PrettyTable

init()  # Fix for windows cmd to highlighted the important keywords

# ######################## Database settings ##########################
'''dbUsername = ""  # database user
dbPassword = ""  # database password
databaseName = ""  # the database name for an ILIAS installation (default: ilias)
databaseIP = ""  # the database name for an ILIAS installation (default: ilias) '''
searchHistory = False  # if true, search the history instead of the active content
# ---------------------- Database settings end ------------------------

# ############################ ILIAS URL ###############################
# url = ""  # the url of the ILIAS website
# -------------------------- ILIAS URL END -----------------------------


filename = "/var/log/ilias_xss/"+str(datetime.datetime.now().date())  # Filename to save the output
outputFile = None  # File to save the output
outputAsCSV = True  # Set whether the file is saved as a csv or a txt file
highlightKeywords = True  # set to False if the criticalKeyword should not be highlighted in the console output
consoleOutput = True  # set to False if the console output should be disabled
displayAll = False  # if true, every possible XSS attempt gets displayed, even on pages where it wont work (e.g. wiki)
displayOnlyCritical = False

# ######################## Vulnerable types ############################
# The types will always be searched for XSS attacks
vulnerable_types = ["gdf", "lm", "cont"]
# --------------------- Vulnerable types end ---------------------------

# ############################# Keywords ###############################
# the keywords get highlighted red in the console output to set the focus on possible critical statements
criticalKeywords = ["javascript", "vbscript", "alert", "script", "iframe", "XSS", "onAbort", "onActivate",
                    "onAfterPrint", "onAfterUpdate",
                    "onBeforeActivate", "onBeforeCopy", "onBeforeCut", "onBeforeDeactivate", "onBeforeEditFocus",
                    "onBeforePaste", "onBeforePrint",
                    "onBeforeUnload", "onBeforeUpdate", "onBegin", "onBlur", "onBounce", "onCellChange", "onChange",
                    "onClick", "onContextMenu",
                    "onControlSelect", "onCopy", "onCut", "onDataAvailable", "onDataSetChanged", "onDataSetComplete",
                    "onDblClick", "onDeactivate",
                    "onDrag", "onDragEnd", "onDragLeave", "onDragEnter", "onDragOver", "onDragDrop", "onDragStart",
                    "onDrop", "onEnd", "onError",
                    "onErrorUpdate", "onFilterChange", "onFinish", "onFocus", "onFocusIn", "onFocusOut", "onHashChange",
                    "onHelp", "onInput", "onKeyDown",
                    "onKeyPress", "onKeyUp", "onLayoutComplete", "onLoad", "onLoseCapture", "onMediaComplete",
                    "onMediaError", "onMessage", "onMouseDown",
                    "onMouseEnter", "onMouseLeave", "onMouseMove", "onMouseOut", "onMouseOver", "onMouseUp",
                    "onMouseWheel", "onMove", "onMoveEnd",
                    "onMoveStart", "onOffline", "onOnline", "onOutOfSync", "onPaste", "onPause", "onPopState",
                    "onProgress", "onPropertyChange",
                    "onReadyStateChange", "onRedo", "onRepeat", "onReset", "onResize", "onResizeEnd", "onResizeStart",
                    "onResume", "onReverse", "onRowsEnter",
                    "onRowExit", "onRowDelete", "onRowInserted", "onScroll", "onSeek", "onSelect", "onSelectionChange",
                    "onSelectStart", "onStart", "onStop",
                    "onStorage", "onSyncRestored", "onSubmit", "onTimeError", "onTrackChange", "onUndo", "onUnload",
                    "onURLFlip", "seekSegmentTime"]
# --------------------------- Keywords end -----------------------------


# ############################# CMD Arguments ###############################
description = "This program searches through the ILIAS database to find potential XSS attacks\nThe default mode " \
              "searches the table 'page_object' and the history mode the table 'page_history' "
parser = argparse.ArgumentParser(description=description,
                                 epilog="Created by: Marius Binal (https://github.com/marius56) and Bastian Buck (https://github.com/bstnbuck)")
groupDatabase = parser.add_argument_group("Database options", "Specify options to connect to the database")
groupDatabase.add_argument("user", help="Set the user to connect to the database")
groupDatabase.add_argument("--password", "-p", action="store_true",
                           help="Enter the password for the user to connect to the database")
groupDatabase.add_argument("--database", default="ilias",
                           help="Set the database for the ILIAS installation. Default: 'ilias'")
groupDatabase.add_argument("--database-ip", default="localhost",
                           help="Set the database IP address for the ILIAS database. Default: 'localhost'")

groupSelection = parser.add_argument_group('Search options', 'Specify option for the database search')
groupSelection.add_argument("--search-all-pages", action="store_true",
                            help="Show every possible XSS attempts, even on pages where the XSS is not possible (e.g. "
                                 "wiki page)")
groupSelection.add_argument("--search-history", action="store_true",
                            help="If set, the program will search though the history instead of the current active "
                                 "content.")

groupOutput = parser.add_argument_group("Output", "Set options for the output")
groupOutput.add_argument("--url", "-u",
                         help="Set a different url for the link output.")
groupOutput.add_argument("--show-complete-entry", action="store_true", default=False,
                         help="Prints the whole entry instead of just the HTML tags.")
groupOutput.add_argument("--show-only-critical", action="store_true",
                         help="Show only entries with critical keywords, e.g. script, onerror, onload")
groupOutput.add_argument("--disable-highlighting", action="store_true",
                         help="Disables the console highlighting for critical keywords")
groupOutput.add_argument("--quiet", "-q", action="store_true", help="Disables the console output")
groupOutput.add_argument("--output", "-o", help="Save the result to a file. Only .csv and .txt are valid extensions!")

groupOutput.add_argument("--noOutput", action="store_true", default=False, help="Do not save a log-File")
groupOutput.add_argument("--use-date", action="store_true", default=False, help="Save the result to a file with "
                                                                                "actual date.")

# #########
# find all users that passwords are locally saved with MD5
md5Password = parser.add_argument_group("Local password", "Set options for finding local saved passwords")
md5Password.add_argument("--check-local-users", action="store_true", default=False, help="Set this to get all locally "
                                                                                         "saved user accounts")
md5Password.add_argument("--show-md5-only", action="store_true", default=False, help="Set this to get local user "
                                                                                     "accounts which only uses "
                                                                                     "md5 saved passwords")
md5Password.add_argument("--show-passwords", action="store_true", default=False, help="Set this to get local user "
                                                                                      "accounts with their passwords")
# #########
args = parser.parse_args()

dbUsername = args.user
databaseName = args.database
databaseIP = args.database_ip
url = args.url

# #########
# find all users that passwords are locally saved with MD5
checkLocalPassword = args.check_local_users
onlyMD5 = args.show_md5_only
showPw = args.show_passwords
# #########

if args.search_history is not None:
    searchHistory = args.search_history

if args.search_all_pages:
    displayAll = True

if args.show_only_critical:
    displayOnlyCritical = True

printCompletePage = args.show_complete_entry

if args.disable_highlighting:
    highlightKeywords = False

if args.quiet:
    consoleOutput = False

# no output
if args.noOutput:
    filename = None

if args.output is not None and args.output != "" and args.use_date is False and not args.noOutput:
    filename = args.output
    if filename.endswith(".csv"):
        outputAsCSV = True
    elif filename.endswith(".txt"):
        outputAsCSV = False
    else:
        print("Error: The output file extension must be .csv or .txt")
        exit()

# usage of actual date
if args.use_date and not args.noOutput:
    fileDateName = datetime.datetime.now().strftime('%d-%m-%Y-%H-%M')
    count = 0
    fileNumToAdd = ""
    if not args.output:
        filename = ""
    else:
        filename = args.output
    while True:
        if exists(filename+fileDateName+fileNumToAdd+".txt"):
            count += 1
            fileNumToAdd = "--"+str(count)
        else:
            filename = filename+fileDateName+fileNumToAdd+".txt"
            break
    outputAsCSV = False

if not args.noOutput and not args.use_date and not args.output:
    count = 0
    fileNumToAdd = ""
    while True:
        if exists(filename+fileNumToAdd+".txt"):
            count += 1
            fileNumToAdd = "--"+str(count)
        else:
            filename = filename+fileNumToAdd+".txt"
            break
    outputAsCSV = False


if args.password is not None and args.password is True:
    dbPassword = getpass()
else:
    dbPassword = "<password-here>"
# --------------------------- CMD Arguments end -----------------------------


# ########################### Regex statements ##############################
# These regex statements are used to search throughout the ILIAS styled content for normal HTML code
# Note: User HTML input is always saved as HTML Entities: "<" is saved as "&lt;", ">" is saved as "&gt;"
# Example:
# <PageContent PCID="[...]"><Paragraph Language="de" Characteristic="Standard">
#   &lt;h1&gt;H1 Headline&lt;/h1&gt;<br/>&lt;img src=x onerror="alert(\'Bild konnte nicht geladen werden\')" /&gt;
# </Paragraph></PageContent>'

# The PHP file "/Services/COPage/classes/class.ilPageObjectGUI.php" convert them back to valid HTML tags, 
# if HTML rendering of the page is activated.
patternParagraphStart = re.compile("<Paragraph[^>]*>")
patternParagraphEnd = re.compile("</Paragraph[^>]*>")
patternHTMLTagStart = re.compile("<([^> ]*)[^>]*>")  # finds every <Tag id="asdf">...</Tag>
patternHTMLTagEnd = None


# -------------------------- Regex statments end ----------------------------


# ############### Function to change back the HTML Entities #################
def removeHTMLEntities(content):
    content = content.replace("&lt;", "<")  # change back &lt; to < for better readability
    content = content.replace("&gt;", ">")  # change back &gt; to > for better readability
    return content


# ------------- Function to change back the HTML Entities end ---------------


# #################### Function to search for HTML tags #####################
def searchForHTML(content):
    soup = BeautifulSoup(content, "html.parser")
    htmlTags = soup.find_all()  # searching for any HTML tags in the content

    usages = []

    if htmlTags is not None:  # if any tag was found, create a list of them and return it
        if displayOnlyCritical:  # if the user toggled that only critical keywords should be shown:
            for element in htmlTags:  # search through the current elements
                element = str(element)
                for keyword in criticalKeywords:
                    if keyword.lower() in element.lower():  # look if any critical keyword is in the current element
                        # if so, add it the the output and stop the search for the current element
                        usages.append(element)
                        break

        else:  # if not only critical keywords should be added, just add every HTML element
            usages = [str(element) for element in htmlTags]

    return usages


# ----------------- Function to search for HTML tags  end -------------------

# #################### Function to create the url path ######################
def createPath(parent_type, ref_id):
    if parent_type == "lm":  # if parent page is a learn module:
        return "/ilias.php?baseClass=ilLMPresentationGUI&ref_id=%s" % (str(ref_id))
    elif parent_type == "wpg":  # if parent page is a wiki, a differend url needs to be used
        return "/goto.php?target=wiki_%s" % (str(ref_id))  # create the url path to the page
    elif parent_type == "blp":  # if parent page is a blog
        return "/ilias.php?ref_id=%s&cmd=preview&cmdClass=ilrepositorygui&cmdNode=tz&baseClass=ilrepositorygui" % (
            str(ref_id))
    elif parent_type == "gdf":  # if parent page is a glossar
        return "/ilias.php?baseClass=ilGlossaryPresentationGUI&ref_id=%s" % (str(ref_id))
    elif parent_type == "copa":
        return "/ilias.php?ref_id=%s&type=copa&item_ref_id=%s&cmd=view&cmdClass=ilobjcontentpagegui&cmdNode=tz:jj" \
               "&baseClass=ilRepositoryGUI" % (
                   str(ref_id), str(ref_id))
    else:  # otherwise:
        return "/goto.php?target=crs_%s" % (str(ref_id))  # create the url path to the page


# ------------------ Function to create the url path end --------------------

def searchForUsages(content):
    usages = []
    paragraphStart = patternParagraphStart.search(content)  # Search for the first "<Paragraph [...]>" tag
    # paragraphEnd = None

    # Repeat the search for opening "<Paragraph [...]>" tag as long as one can be found
    while paragraphStart is not None:
        paragraphEnd = patternParagraphEnd.search(content,
                                                  paragraphStart.end())  # search for the closing "</Paragraph>" tag

        if paragraphEnd is not None:  # if a closing </Paragraph> was found
            currentContent = content[paragraphStart.end():paragraphEnd.start()]

            if printCompletePage:  # if the user wants to get the complete page:
                usages.append(currentContent)
            else:  # otherwise search the page for HTML tags:
                usages += searchForHTML(currentContent)

            # New point to start is the end of the closing </Paragraph>
            paragraphStart = patternParagraphStart.search(content, paragraphEnd.end())

        else:  # if no closing </Paragraph> was found, search through the rest of the text and exit the while loop
            usages += searchForHTML(content[paragraphStart.end():])
            paragraphStart = None
    return usages


# ############################## File output ################################
def prepareFile(searchHistory):
    # writer = None
    if not searchHistory:
        if outputAsCSV:
            outFile = open(filename, "w")  # open the file ...
            writers = csv.writer(outFile)  # ... and create a csv writer to write the output to the file
            writers.writerow(["Name", "Username", "E-Mail", "Created", "Last change", "Last changed by", "Link to page",
                              "Content"])  # start with the headlines of the columns
        else:
            writers = PrettyTable()
            writers.field_names = ["Name", "Username", "E-Mail", "Created", "Last change", "Last changed by",
                                   "Link to page", "Content"]
    else:
        if outputAsCSV:
            outFile = open(filename, "w")  # open the file ...
            writers = csv.writer(outFile)  # ... and create a csv writer to write the output to the file
            writers.writerow(["Name", "Username", "E-Mail", "Timestamp", "PageID", "Nr. of edit", "Link to page",
                              "Content"])  # start with the headlines of the columns
        else:
            writers = PrettyTable()
            writers.field_names = ["Name", "Username", "E-Mail", "Timestamp", "PageID", "Nr. of edit", "Link to page",
                                   "Content"]
    return writers


# ---------------------------- File output end ------------------------------


# ############################# Database part ###############################
def fetchData(searchHistory):
    database = db.connect(databaseIP, dbUsername, dbPassword, databaseName)
    cursor = database.cursor()

    if not searchHistory:
        # Removed column "page.create_user" and "page.parent_id" from the query because they were not necessary 
        cursor.execute("""SELECT page.page_id, page.parent_type, ref.ref_id, ref.deleted, page.content, user.login, user.firstname, user.lastname, user.email, page.active, page.created, page.last_change, changedBy.login  FROM page_object page
        INNER JOIN usr_data user ON page.create_user = user.usr_id 
        INNER JOIN usr_data changedBy ON page.last_change_user = changedBy.usr_id 
        INNER JOIN object_reference ref ON ref.obj_id = page.parent_id 
        WHERE page.content LIKE '%&lt;%';""")
    else:
        cursor.execute("""SELECT page.page_id, page.parent_type, ref.ref_id, ref.deleted, page.content, user.login, user.firstname, user.lastname, user.email, page.hdate, page.nr FROM page_history page
        INNER JOIN usr_data user ON page.user_id = user.usr_id 
        INNER JOIN object_reference ref ON ref.obj_id = page.parent_id 
        WHERE page.content LIKE '%&lt;%';""")

    return cursor


# -------------------------- Database part end ------------------------------

def parseData(data, searchHistory, writers=None):
    counterFound = 0  # Variable to count the amount of pages with potential XSS attacks

    if not searchHistory:
        # ################### Parsing the database response #########################
        # Variables:
        # page_id = id of page; ref_id = id for the url -> crs_[ref_id], 
        # parent_type = page type (e.g. content, wiki) of the parent page
        # deleted = whether the "ref_id" page containing the content was deleted
        # content = ILIAS styled content; active = whether the page is active 
        for page_id, parent_type, ref_id, deleted, content, username, firstname, lastname, email, active, created, last_change, last_change_by in data:
            # its only possible to inject javascript an html code on a page overview
            # therefore we can skip for example items of the type "wpg" because that's a page
            # in a wiki, where HTML rendering isn't allowed at all

            if parent_type not in vulnerable_types and displayAll is not False:
                continue

            content = removeHTMLEntities(content)
            usages = searchForUsages(content)

            if len(usages) == 0:  # if no html tag were found, skip this entry
                continue

            counterFound += 1

            path = createPath(parent_type, ref_id)

            link = "%s%s" % (url, path)
            name = "%s %s" % (firstname, lastname)

            if filename is not None and filename != "":
                linkText = link
                # if the page which contained the element was deleted, add a note into the link column
                if deleted is not None:
                    linkText += "\n==> The page (ref_id=%d) which included this element (page_id=%d), was deleted on " \
                                "the %s!" % (
                                    ref_id, page_id, deleted)
                if active != 1:  # if the element is not active, add a note into the link column
                    linkText += "\n==> This element (page_id=%d) is not active at the moment (Row active=0 in the " \
                                "database)" % page_id

                if writers is not None:
                    if outputAsCSV:
                        writers.writerow([name, username, email, created, last_change, last_change_by, linkText,
                                          "\n\r".join(usages)])
                    else:
                        writers.add_row(
                            [name, username, email, created, last_change, last_change_by, linkText, usages[0]])
                        if len(usages) > 1:
                            for usage in usages[1:]:
                                writers.add_row(["", "", "", "", "", "", "", usage])

                        writers.add_row(["", "", "", "", "", "", "", ""])

            if consoleOutput:
                print("#################################### Found - Start ####################################")
                print("Name: %s | Username: %s | E-Mail: %s" % (name, username, email))
                print("Created: %s | Last change: %s (by: %s)" % (created, last_change, last_change_by))
                print("Link to the page: %s" % link)

                colorRed = ""
                colorReset = ""

                if highlightKeywords:
                    colorRed = Fore.LIGHTRED_EX
                    colorReset = Style.RESET_ALL

                # if the page which contained the element was deleted, add a note to the console output
                if deleted is not None:
                    print(
                        "%sThe page (ref_id=%d) which included this element (page_id=%d), was deleted on the %s!%s" % (
                            colorRed, ref_id, page_id, deleted, colorReset))
                if active != 1:  # if the element is not active, add a note to the console output
                    print(
                        "%sThis element (page_id=%d) is not active at the moment (Row active=0 in the database)%s" % (
                            colorRed, page_id, colorReset))
                # if (parent_type == "wpg"): # if the element is inside a wiki page
                #    print("%sThe XSS is only executed on the property page of the table.%s" % (colorRed, colorReset))

                if highlightKeywords:
                    for i in range(len(usages)):
                        # highlight any critical keyword, to give a better overview when looking
                        # over the console output
                        for keyword in criticalKeywords:
                            occurrences = re.findall(keyword, usages[i],
                                                     re.IGNORECASE)  # using regex to be able to search case insensitive
                            for occurrence in occurrences:
                                usages[i] = usages[i].replace(occurrence, "%s%s%s" % (
                                    Fore.RED, occurrence,
                                    Style.RESET_ALL))  # using colorama to paint critical keywords red

                print("Content: \n%s" % ("\n".join(usages)))
                print("------------------------------------  Found - End  ------------------------------------\n\n")
        # ----------------- Parsing the database response end -----------------------
    else:
        # ################### Parsing the database response #########################
        # Variables:
        # page_id = id of page; ref_id = id for the url -> crs_[ref_id], 
        # parent_type = page type (e.g. content, wiki) of the parent page
        # deleted = whether the "ref_id" page containing the content was deleted
        # content = ILIAS styled content; active = whether the page is active 
        # date = Timestamp when the entry was changed
        # nr = Number of change of the page

        # page.page_id, page.parent_type, ref.ref_id, ref.deleted, page.content, user.login, user.firstname,
        # user.lastname, user.email, page.hdate, page.nr
        for page_id, parent_type, ref_id, deleted, content, username, firstname, lastname, email, date, nr in data:
            # its only possible to inject javascript an html code on a page overview
            # therefore we can skip for example items of the type "wpg" because that's a page
            # in a wiki, where HTML rendering isn't allowed at all
            if parent_type not in vulnerable_types and displayAll == False:
                continue

            content = removeHTMLEntities(content)
            usages = searchForUsages(content)

            if len(usages) == 0:  # if no html tag were found, skip this entry
                continue

            counterFound += 1

            path = createPath(parent_type, ref_id)

            link = "%s%s" % (url, path)
            name = "%s %s" % (firstname, lastname)

            if filename is not None and filename != "":
                linkText = link
                # if the page which contained the element was deleted, add a note into the link column
                if deleted is not None:
                    linkText += "\n==> The page (ref_id=%d) which included this element (page_id=%d), was deleted on " \
                                "the %s!" % (ref_id, page_id, deleted)

                if writers is not None:
                    if outputAsCSV:
                        writers.writerow([name, username, email, date, page_id, nr, linkText, "\n\r".join(usages)])
                    else:
                        writers.add_row([name, username, email, date, page_id, nr, linkText, usages[0]])
                        if len(usages) > 1:
                            for usage in usages[1:]:
                                writers.add_row(["", "", "", "", "", "", "", usage])

                        writers.add_row(["", "", "", "", "", "", "", ""])

            if consoleOutput:
                print("#################################### Found - Start ####################################")
                print("Name: %s | Username: %s | E-Mail: %s" % (name, username, email))
                print("Timestamp: %s | PageID: %d | Nr. of edit: %d" % (date, page_id, nr))
                print("Link to the page: %s" % link)

                colorRed = ""
                colorReset = ""

                if highlightKeywords:
                    colorRed = Fore.LIGHTRED_EX
                    colorReset = Style.RESET_ALL

                # if the page which contained the element was deleted, add a note to the console output
                if deleted is not None:
                    print(
                        "%sThe page (ref_id=%d) which included this element (page_id=%d), was deleted on the %s!%s" % (
                            colorRed, ref_id, page_id, deleted, colorReset))

                if highlightKeywords:
                    for i in range(len(usages)):
                        # highlight any critical keyword, to give a better overview when looking
                        # over the console output
                        for keyword in criticalKeywords:
                            occurrences = re.findall(keyword, usages[i],
                                                     re.IGNORECASE)  # using regex to be able to search case insensitive
                            for occurrence in occurrences:
                                usages[i] = usages[i].replace(occurrence, "%s%s%s" % (
                                    Fore.RED, occurrence,
                                    Style.RESET_ALL))  # using colorama to paint critical keywords red

                print("Content: \n%s" % ("\n".join(usages)))
                print("------------------------------------  Found - End  ------------------------------------\n\n")
        # ----------------- Parsing the database response end -----------------------

    if consoleOutput:
        print("#######################################################################################")
        print("####################################### Finished ######################################")
        print("#######################################################################################")
        print("=> Number of entries with potential XSS attacks: %d" % counterFound)

    if writers is not None and outputAsCSV is False:
        outFile = open(filename, "w")  # open the file ...
        outFile.write(writers.get_string())
        outFile.close()


# #########
# find all users that passwords are locally saved with MD5
def checkLocalPasswords():
    database = db.connect(databaseIP, dbUsername, dbPassword, databaseName)
    cursor = database.cursor()
    if showPw:
        if onlyMD5:
            cursor.execute("""SELECT usr_id, login, passwd, firstname, lastname, email FROM usr_data 
            WHERE CHAR_LENGTH(passwd) = 32;""")
        else:
            cursor.execute("""SELECT usr_id, login, passwd, firstname, lastname, email FROM usr_data;""")
        return cursor
    else:
        if onlyMD5:
            cursor.execute("""SELECT usr_id, login, firstname, lastname, email FROM usr_data 
            WHERE CHAR_LENGTH(passwd) = 32;""")  # a MD5 hash is 32 bytes long
        else:
            cursor.execute("""SELECT usr_id, login, firstname, lastname, email FROM usr_data;""")
        return cursor


def prepareFileL():
    if showPw:
        if outputAsCSV:
            out = open(filename, "w")  # open the file ...
            writers = csv.writer(out)  # ... and create a csv writer to write the output to the file
            writers.writerow(["User-ID", "Username", "Password", "Firstname", "Lastname", "E-Mail"])
        else:
            writers = PrettyTable()
            writers.field_names = ["User-ID", "Username", "Password", "Firstname", "Lastname", "E-Mail"]
        return writers
    else:
        if outputAsCSV:
            out = open(filename, "w")  # open the file ...
            writers = csv.writer(out)  # ... and create a csv writer to write the output to the file
            writers.writerow(["User-ID", "Username", "Firstname", "Lastname", "E-Mail"])
        else:
            writers = PrettyTable()
            writers.field_names = ["User-ID", "Username", "Firstname", "Lastname", "E-Mail"]
        return writers


def parseDataL(data, writers=None):
    count = 0
    if showPw:
        for user_id, login, password, firstname, lastname, email in data:
            if filename is not None and filename != "" and writers is not None:
                if outputAsCSV:
                    writers.writerow([user_id, login, password, firstname, lastname, email, "\n\r".join("")])
                else:
                    writers.add_row([user_id, login, password, firstname, lastname, email])
                    writers.add_row(["", "", "", "", "", ""])
            if consoleOutput:
                print("Next-User: ", count)
                print("User-ID %i | Username: %s | Password: %s |" % (user_id, login, password))
                print("Firstname: %s | Lastname: %s |  E-Mail: %s" % (firstname, lastname, email))
                print()
            count += 1
    else:
        for user_id, login, firstname, lastname, email in data:
            if filename is not None and filename != "" and writers is not None:
                if outputAsCSV:
                    writers.writerow([user_id, login, firstname, lastname, email, "\n\r".join("")])
                else:
                    writers.add_row([user_id, login, firstname, lastname, email])
                    writers.add_row(["", "", "", "", ""])
            if consoleOutput:
                print("Next-User: ", count)
                print("User-ID %i | Username: %s" % (user_id, login))
                print("Firstname: %s | Lastname: %s |  E-Mail: %s" % (firstname, lastname, email))
                print()
            count += 1
    if consoleOutput:
        if onlyMD5:
            print("=> Number of entries with MD5 passwords: %d" % count)
        else:
            print("=> Number of local saved user accounts: %d" % count)
    if writers is not None and outputAsCSV is False:
        outfile = open(filename, "w")  # open the file ...
        outfile.write(writers.get_string())
        outfile.close()


# ##########

# function to detect old files
def removeOldLogs():
    for root, _, files in os.walk(filePath):
        for file in files:
            file = os.path.abspath(os.path.join(root, file))
            _, onlyFile = os.path.split(file)
            try:
                if datetime.datetime.now().date()-datetime.datetime.strptime(onlyFile, "%Y-%m-%d").date() >= \
                        datetime.timedelta(days=14):
                    os.remove(file)
            except:
                continue


if __name__ == "__main__":
    if checkLocalPassword and filename is not None and filename != "":
        filePath = "/var/log/ilias_xss/"
        if not args.output:
            if not exists(filePath):
                os.makedirs(filePath)
            removeOldLogs()
        writer = prepareFileL()
        parseDataL(checkLocalPasswords(), writer)

    elif checkLocalPassword and filename is None:
        parseDataL(checkLocalPasswords())
    else:
        if filename is not None and filename != "":  # if an output file was specified
            filePath = "/var/log/ilias_xss/"
            if not args.output:
                if not exists(filePath):
                    os.makedirs(filePath)
                removeOldLogs()
            writer = prepareFile(searchHistory)
            parseData(fetchData(searchHistory), searchHistory, writer)
        else:
            parseData(fetchData(searchHistory), searchHistory)


# check local users with md5 passwords, output as self named file -> python ILIAS_XSS_Database_search.py --password --database={db-name}
#       --database-ip={ilias-ip} --check-local-users --show-md5-only --output=<filename> ilias

# check local users with md5 passwords, automatically create and delete logs in /var/log/ilias_xss/ -> python ILIAS_XSS_Database_search.py --password --database={db-name}
#       --database-ip={ilias-ip} --check-local-users --show-md5-only ilias

# check local users with md5 passwords, no output will be generated -> python ILIAS_XSS_Database_search.py --password --database={db-name}
#       --database-ip={ilias-ip} --check-local-users --show-md5-only --noOutput ilias

# check local users: python ILIAS_XSS_Database_search.py --password --database={db-name} --database-ip={db-ip}
#       --check-local-users --output={output-file} ilias

# add --show-passwords to get the passwords of the users

# add --quiet for no console output

# add for full XSS output: --search-all-pages --search-history --show-complete-entry

# add for search only critical: --search-all-pages --search-history --show-only-critical

# add this to create files with actual date (do not work with --output): --use-date
